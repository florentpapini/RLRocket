{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8733053-0419-4b89-bc81-ab78ccf07f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rlgym.envs import Match\n",
    "from rlgym.utils.action_parsers import DiscreteAction\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor, VecNormalize, VecCheckNan\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.logger import Logger, TensorBoardOutputFormat\n",
    "\n",
    "from rlgym.utils.obs_builders import AdvancedObs\n",
    "from rlgym.utils.state_setters import DefaultState\n",
    "from rlgym.utils.terminal_conditions.common_conditions import TimeoutCondition, GoalScoredCondition\n",
    "from rlgym_tools.sb3_utils import SB3MultipleInstanceEnv\n",
    "from rlgym.utils.reward_functions.common_rewards.misc_rewards import EventReward\n",
    "from rlgym.utils.reward_functions.common_rewards.player_ball_rewards import VelocityPlayerToBallReward\n",
    "from rlgym.utils.reward_functions.common_rewards.ball_goal_rewards import VelocityBallToGoalReward\n",
    "from rlgym.utils.reward_functions import CombinedReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb598c-426c-49a3-971d-309b3a1602d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs\\PPO_229\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 450         |\n",
      "|    ep_rew_mean     | -0.14412375 |\n",
      "| time/              |             |\n",
      "|    fps             | 673         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 148         |\n",
      "|    total_timesteps | 3000000     |\n",
      "------------------------------------\n",
      "Logging to logs\\PPO_229\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 450         |\n",
      "|    ep_rew_mean     | -0.14412375 |\n",
      "| time/              |             |\n",
      "|    fps             | 705         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 141         |\n",
      "|    total_timesteps | 3100000     |\n",
      "------------------------------------\n",
      "Logging to logs\\PPO_229\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 450         |\n",
      "|    ep_rew_mean     | -0.14412375 |\n",
      "| time/              |             |\n",
      "|    fps             | 648         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 154         |\n",
      "|    total_timesteps | 3200000     |\n",
      "------------------------------------\n",
      "Logging to logs\\PPO_229\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 450         |\n",
      "|    ep_rew_mean     | -0.14412375 |\n",
      "| time/              |             |\n",
      "|    fps             | 679         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 147         |\n",
      "|    total_timesteps | 3300000     |\n",
      "------------------------------------\n",
      "Logging to logs\\PPO_229\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 450         |\n",
      "|    ep_rew_mean     | -0.14412375 |\n",
      "| time/              |             |\n",
      "|    fps             | 705         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 141         |\n",
      "|    total_timesteps | 3400000     |\n",
      "------------------------------------\n",
      "Logging to logs\\PPO_229\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 450         |\n",
      "|    ep_rew_mean     | -0.14412375 |\n",
      "| time/              |             |\n",
      "|    fps             | 709         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 140         |\n",
      "|    total_timesteps | 3500000     |\n",
      "------------------------------------\n",
      "Logging to logs\\PPO_229\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 450         |\n",
      "|    ep_rew_mean     | -0.14412375 |\n",
      "| time/              |             |\n",
      "|    fps             | 692         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 144         |\n",
      "|    total_timesteps | 3600000     |\n",
      "------------------------------------\n",
      "Logging to logs\\PPO_229\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  # Required for multiprocessing\n",
    "    frame_skip = 8          # Number of ticks to repeat an action\n",
    "    half_life_seconds = 5   # Easier to conceptualize, after this many seconds the reward discount is 0.5\n",
    "\n",
    "    fps = 120 / frame_skip\n",
    "    gamma = np.exp(np.log(0.5) / (fps * half_life_seconds))  # Quick mafs\n",
    "    target_steps = 100_000\n",
    "    agents_per_match = 4\n",
    "    num_instances = 1\n",
    "    steps = target_steps // (num_instances * agents_per_match)\n",
    "    batch_size = steps\n",
    "    waitload = 90\n",
    "\n",
    "    # print(f\"fps={fps}, gamma={gamma})\")\n",
    "\n",
    "\n",
    "    def get_match():  # Need to use a function so that each instance can call it and produce their own objects\n",
    "        return Match(\n",
    "            team_size=2,  # 3v3 to get as many agents going as possible, will make results more noisy\n",
    "            tick_skip=frame_skip,\n",
    "            reward_function=CombinedReward(\n",
    "            (\n",
    "                 VelocityPlayerToBallReward(),\n",
    "                 VelocityBallToGoalReward(),\n",
    "                 EventReward(\n",
    "                     team_goal=100.0,\n",
    "                     concede=-100.0,\n",
    "                     shot=5.0,\n",
    "                     save=30.0,\n",
    "                     demo=10.0,\n",
    "                 ),\n",
    "            ),\n",
    "            (0.1, 1.0, 1.0)),\n",
    "            spawn_opponents=True,\n",
    "            terminal_conditions=[TimeoutCondition(round(fps * 30)), GoalScoredCondition()],  # Some basic terminals\n",
    "            obs_builder=AdvancedObs(),  # Not that advanced, good default\n",
    "            state_setter=DefaultState(),  # Resets to kickoff position\n",
    "            action_parser=DiscreteAction()  # Discrete > Continuous don't @ me\n",
    "        )\n",
    "\n",
    "    env = SB3MultipleInstanceEnv(get_match, num_instances, wait_time=waitload)            # Start 1 instances, waiting 60 seconds between each\n",
    "    env = VecCheckNan(env)                                # Optional\n",
    "    #env = VecMonitor(env)                                 # Recommended, logs mean reward and ep_len to Tensorboard\n",
    "    env = VecNormalize(env, norm_obs=False, gamma=gamma)  # Highly recommended, normalizes rewards\n",
    "\n",
    "    try:\n",
    "        model = PPO.load(\"models/Crook\",\n",
    "                         env,\n",
    "                         device=\"auto\",\n",
    "                         tensorboard_log=\"logs\",\n",
    "                         verbose=1\n",
    "                         )\n",
    "    except:\n",
    "        from torch.nn import Tanh\n",
    "        policy_kwargs = dict(\n",
    "            activation_fn=Tanh,\n",
    "            net_arch=[512, 512, dict(pi=[256, 256, 256], vf=[256, 256, 256])],\n",
    "        )\n",
    "        model = PPO(\n",
    "            MlpPolicy,\n",
    "            env,\n",
    "            n_epochs=1,  # PPO calls for multiple epochs\n",
    "            #policy_kwargs=policy_kwargs,\n",
    "            learning_rate=5e-5,  # Around this is fairly common for PPO\n",
    "            ent_coef=0.01,  # From PPO Atari\n",
    "            vf_coef=1.,  # From PPO Atari\n",
    "            gamma=gamma,  # Gamma as calculated using half-life\n",
    "            verbose=1,  # Print out all the info as we're going\n",
    "            batch_size=batch_size,  # Batch size as high as possible within reason\n",
    "            n_steps=steps,  # Number of steps to perform before optimizing network\n",
    "            tensorboard_log=\"logs\",  # `tensorboard --logdir out/logs` in terminal to see graphs\n",
    "            device=\"auto\"  # Uses GPU if available\n",
    "        )\n",
    "    #model.set_logger(TensorBoardOutputFormat(\"logs\"))\n",
    "\n",
    "\n",
    "    # Save model every so often\n",
    "    # Divide by num_envs (number of agents) because callback only increments every time all agents have taken a step\n",
    "    # This saves to specified folder with a specified name\n",
    "    callback = CheckpointCallback(round(5_000_000 / env.num_envs), save_path=\"models\", name_prefix=\"rl_model\")\n",
    "\n",
    "    while True:\n",
    "        model.learn(1000, callback=callback, reset_num_timesteps=False)\n",
    "        model.save(\"models/Crook\")\n",
    "        model.save(f\"mmr_models/{model.num_timesteps}\")\n",
    "\n",
    "    # Now, if one wants to load a trained model from a checkpoint, use this function\n",
    "    # This will contain all the attributes of the original model\n",
    "    # Any attribute can be overwritten by using the custom_objects parameter,\n",
    "    # which includes n_envs (number of agents), which has to be overwritten to use a different amount\n",
    "    # model = PPO.load(\n",
    "    #     \"policy/rl_model_1000002_steps.zip\",\n",
    "    #     env,\n",
    "    #     custom_objects=dict(n_envs=env.num_envs, _last_obs=None),  # Need this to change number of agents\n",
    "    #     device=\"auto\",  # Need to set device again (if using a specific one)\n",
    "    #     force_reset=True  # Make SB3 reset the env so it doesn't think we're continuing from last state\n",
    "    # )\n",
    "    # env.close()\n",
    "    # Use reset_num_timesteps=False to keep going with same logger/checkpoints\n",
    "    # model.learn(100_000_000, callback=callback, reset_num_timesteps=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6e3b6-a6e8-4f43-b74a-29b01a660a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate rlgymenv\n",
    "#cd OneDrive/Bureau/machine learning/rl2v2/\n",
    "#tensorboard --logdir ./logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
